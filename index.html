<!DOCTYPE html>
<html lang="en">

<head>
    <script>
        function togglediv(id) {
            var div = document.getElementById(id);
            div.style.display = (div.style.display == "none" || div.style.display == "") ? "block" : "none";
        }
    </script>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-147178996-2"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', 'UA-147178996-2');
    </script>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Personal webpage.">
    <meta name="author" content="Clemens Eppner">

    <title>Clemens Eppner</title>

    <!-- Begin favicon settings -->
    <link rel="shortcut icon" href="favicon/favicon.ico" type="image/x-icon">
    <link rel="icon" sizes="16x16 32x32 64x64" href="favicon/favicon.ico">
    <link rel="icon" type="image/png" sizes="196x196" href="favicon/favicon-192.png">
    <link rel="icon" type="image/png" sizes="160x160" href="favicon/favicon-160.png">
    <link rel="icon" type="image/png" sizes="96x96" href="favicon/favicon-96.png">
    <link rel="icon" type="image/png" sizes="64x64" href="favicon/favicon-64.png">
    <link rel="icon" type="image/png" sizes="32x32" href="favicon/favicon-32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="favicon/favicon-16.png">
    <link rel="apple-touch-icon" href="favicon/favicon-57.png">
    <link rel="apple-touch-icon" sizes="114x114" href="favicon/favicon-114.png">
    <link rel="apple-touch-icon" sizes="72x72" href="favicon/favicon-72.png">
    <link rel="apple-touch-icon" sizes="144x144" href="favicon/favicon-144.png">
    <link rel="apple-touch-icon" sizes="60x60" href="favicon/favicon-60.png">
    <link rel="apple-touch-icon" sizes="120x120" href="favicon/favicon-120.png">
    <link rel="apple-touch-icon" sizes="76x76" href="favicon/favicon-76.png">
    <link rel="apple-touch-icon" sizes="152x152" href="favicon/favicon-152.png">
    <link rel="apple-touch-icon" sizes="180x180" href="favicon/favicon-180.png">
    <meta name="msapplication-TileColor" content="#FFFFFF">
    <meta name="msapplication-TileImage" content="favicon/favicon-144.png">
    <meta name="msapplication-config" content="favicon/browserconfig.xml">
    <!-- End favicon settings -->

    <link rel="stylesheet" type="text/css" href="./fonts/fonts.css" />
    <link rel="stylesheet" type="text/css" href="./dist/wing.css" />
    <link rel="stylesheet" type="text/css" href="./css/styles.css" />

    <link rel="stylesheet" href="./css/font-awesome.min.css" />
    <link rel="stylesheet" href="./css/academicons.min.css" />

    <base target="_blank">
</head>

<body>
    <section class="banner container">
        <div class="banner-title">
            <img id="bannerportrait" src="./img/delauney.png" alt="Portrait" height="125" width="125">
            <div class="right-col-center">
                <h3 class="left">Clemens Eppner</h3>
                <p><span class="link_color">ceppner at nvidia dot com</span></p>
            </div>
        </div>
        <div class="banner-text">
            <p>I am a Research Scientist in the <a
                    href="https://blogs.nvidia.com/blog/2019/01/11/nvidia-seattle-ai-robotics-research-lab/">Seattle
                    Robotics Lab</a> at <a href="https://www.nvidia.com/en-us/research/">NVIDIA Research</a> led by <a
                    href="http://homes.cs.washington.edu/~fox/">Dieter Fox</a>. I am interested in the problem space of
                grasping and manipulation, including aspects of planning, control, and perception. Although this domain
                may seem specific, I believe that the hybrid systems nature of grasping and manipulation is echoed in a
                wide range of important decision-making problems. Furthermore, I consider robotics to be a fundamentally
                empirical enterprise. Building systems that alter our physical world is, therefore, an integral part of
                my work.</p>
            <p>Before joining NVIDIA, I received my Ph.D. at the <a href="http://www.robotics.tu-berlin.de">Robotics and
                    Biology Lab</a> at TU Berlin under the supervision of <a
                    href="https://www.robotics.tu-berlin.de/menue/team/oliver_brock">Oliver Brock</a>. While studying at
                the University of Freiburg, I wrote my Master's thesis at the <a
                    href="http://ais.informatik.uni-freiburg.de/index_en.php">Autonomous Intelligent Systems</a> lab
                headed by <a href="http://www2.informatik.uni-freiburg.de/~burgard/">Wolfram Burgard</a> and worked at
                <a href="http://ais.uni-bonn.de/behnke/">Sven Behnke</a>'s <a
                    href="http://nimbro.net/index_NimbRo.html">Humanoid Robots Group</a>. I also enjoyed a research stay
                at <a href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a>'s <a
                    href="http://rll.berkeley.edu/">Robot Learning Lab</a> at UC Berkeley.
            </p>
            <p style="margin-bottom: 0px;">
                <a href="https://aemail.com/wp0M" aria-label="Email link">
                    <!---->
                    <i class="fa fa-envelope-square ai-2x"></i>
                </a>
                <!---->
                <a href="https://www.linkedin.com/in/clemens-eppner/" aria-label="LinkedIn link">
                    <i class="fa fa-linkedin-square ai-2x"></i>
                </a>
                <a href="https://twitter.com/clembow" aria-label="Twitter link">
                    <i class="fa fa-twitter-square ai-2x"></i>
                </a>
                <a href="https://github.com/clemense" aria-label="Github link">
                    <i class="fa fa-github-square ai-2x"></i>
                </a>

                <a href="https://scholar.google.com/citations?user=zMw7PF8AAAAJ" aria-label="Google scholar link">
                    <i class="ai ai-fw ai-google-scholar-square ai-2x"></i>
                </a>
            </p>
        </div>
    </section>

    <section class="container">
        <!-- <h3 class="header">Preprints</h3>
         -->

        <h3 class="header">Refereed Conference &amp; Journal Publications</h3>
        
            <div class="publication">
                <div class="col-image"><img src="thumbnails/huang2021defgraspsim.png" width="100" alt="" /></div>
                <div class="col-rest">
                    <h5>DefGraspSim: Simulation-based Grasping of 3D Deformable Objects</h5>
                    <div class="authors"><a href="https://scholar.google.com/citations?user=04Ta98kAAAAJ&hl=en">Isabella Huang</a>, <a href="https://scholar.google.com/citations?user=M3NuG7AAAAAJ&hl=en">Yashraj Narang</a>, <b>Clemens Eppner</b>, <a href="https://balakumar-s.github.io/">Balakumar Sundaralingam</a>, <a href="http://mmacklin.com">Miles Macklin</a>, <a href="https://robot-learning.cs.utah.edu/thermans">Tucker Hermans</a> and <a href="https://homes.cs.washington.edu/~fox/">Dieter Fox</a></div>
                    <div class="booktitle">R:SS 2021 Workshop on Deformable Object Simulation in Robotics. July 2021.</div>
                    <div>[<a href="#" title="Show Abstract" onclick="javascript:togglediv('huang2021defgraspsim_abstract'); return false;">Abstract</a>] [<a title="Show BibTex" href="#" onclick="javascript:togglediv('huang2021defgraspsim_bibtex'); return false;">BibTex</a>] [<a href="https://arxiv.org/pdf/2107.05778.pdf">PDF</a>]  [<a href="https://sites.google.com/nvidia.com/defgraspsim">Website</a>] [<a href="https://github.com/NVlabs/DefGraspSim">Code &#9733;<small>61</small></a>] [<span class="link_color">Press:</span> <a href="https://techxplore.com/news/2021-09-defgraspsim-pipeline-robotic-grasping-3d.html">TechXplore</a>]<br> [<a href="https://sites.google.com/nvidia.com/do-sim/posters?authuser=0">Best Workshop Paper Award</a>]</div>
                    <div class="bibtex" id="huang2021defgraspsim_bibtex"><br />@inproceedings{huang2021defgraspsim,<br />&nbsp;&nbsp;title = {DefGraspSim: Simulation-based Grasping of 3D Deformable Objects},<br />&nbsp;&nbsp;author = {Huang, Isabella and Narang, Yashraj and Eppner, Clemens and Sundaralingam, Balakumar and Macklin, Miles and Hermans, Tucker and Fox, Dieter},<br />&nbsp;&nbsp;booktitle = {R:SS 2021 Workshop on Deformable Object Simulation in Robotics},<br />&nbsp;&nbsp;year = {2021}<br />}</div>
                    <div class="abstract" id="huang2021defgraspsim_abstract">Robotic grasping of 3D deformable objects (e.g., fruits/vegetables, internal organs, bottles/boxes) is critical for real-world applications such as food processing, robotic surgery, and household automation. However, developing grasp strategies for such objects is uniquely challenging. In this work, we efficiently simulate grasps on a wide range of 3D deformable objects using a GPU-based implementation of the corotational finite element method (FEM). To facilitate future research, we open-source our simulated dataset (34 objects, 1e5 Pa elasticity range, 6800 grasp evaluations, 1.1M grasp measurements), as well as a code repository that allows researchers to run our full FEM-based grasp evaluation pipeline on arbitrary 3D object models of their choice. We also provide a detailed analysis on 6 object primitives. For each primitive, we methodically describe the effects of different grasp strategies, compute a set of performance metrics (e.g., deformation, stress) that fully capture the object response, and identify simple grasp features (e.g., gripper displacement, contact area) measurable by robots prior to pickup and predictive of these performance metrics. Finally, we demonstrate good correspondence between grasps on simulated objects and their real-world counterparts.</div>
                </div>
            </div>
            
            <div class="publication">
                <div class="col-image"><img src="thumbnails/islam2020alternative.png" width="100" alt="" /></div>
                <div class="col-rest">
                    <h5>Alternative Paths Planner (APP) for Provably Fixed-time Manipulation Planning in Semi-structured Environments</h5>
                    <div class="authors"><a href="http://www.ri.cmu.edu/person.html?person_id=3176">Fahad Islam</a>, <a href="https://cpaxton.github.io/">Christopher Paxton</a>, <b>Clemens Eppner</b>, <a href="http://www.gnarlydesign.io/">Bryan Peele</a>, <a href="https://www.cs.cmu.edu/~maxim/">Maxim Likhachev</a> and <a href="https://homes.cs.washington.edu/~fox/">Dieter Fox</a></div>
                    <div class="booktitle">Proceedings of the IEEE International Conference on Robotics and Automation (ICRA). May 2021.</div>
                    <div>[<a href="#" title="Show Abstract" onclick="javascript:togglediv('islam2020alternative_abstract'); return false;">Abstract</a>] [<a title="Show BibTex" href="#" onclick="javascript:togglediv('islam2020alternative_bibtex'); return false;">BibTex</a>] [<a href="https://arxiv.org/pdf/2012.14970">PDF</a>] </div>
                    <div class="bibtex" id="islam2020alternative_bibtex"><br />@inproceedings{islam2020alternative,<br />&nbsp;&nbsp;title = {Alternative Paths Planner (APP) for Provably Fixed-time Manipulation Planning in Semi-structured Environments},<br />&nbsp;&nbsp;author = {Islam, Fahad and Paxton, Christopher and Eppner, Clemens and Peele, Bryan and Likhachev, Maxim and Fox, Dieter},<br />&nbsp;&nbsp;booktitle = {Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)},<br />&nbsp;&nbsp;year = {2021}<br />}</div>
                    <div class="abstract" id="islam2020alternative_abstract">In many applications, including logistics and manufacturing, robot manipulators operate in semi-structured environments alongside humans or other robots. These environments are largely static, but they may contain some movable obstacles that the robot must avoid. Manipulation tasks in these applications are often highly repetitive, but require fast and reliable motion planning capabilities, often under strict time constraints. Existing preprocessing-based approaches are beneficial when the environments are highly-structured, but their performance degrades in the presence of movable obstacles, since these are not modelled a priori. We propose a novel preprocessing-based method called Alternative Paths Planner (APP) that provides provably fixed-time planning guarantees in semi-structured environments. APP plans a set of alternative paths offline such that, for any configuration of the movable obstacles, at least one of the paths from this set is collision-free. During online execution, a collision-free path can be looked up efficiently within a few microseconds. We evaluate APP on a 7 DoF robot arm in semi-structured domains of varying complexity and demonstrate that APP is several orders of magnitude faster than state-of-the-art motion planners for each domain. We further validate this approach with real-time experiments on a robotic manipulator.</div>
                </div>
            </div>
            
            <div class="publication">
                <div class="col-image"><img src="thumbnails/eppner2020acronym.png" width="100" alt="" /></div>
                <div class="col-rest">
                    <h5>ACRONYM: A Large-Scale Grasp Dataset Based on Simulation</h5>
                    <div class="authors"><b>Clemens Eppner</b>, <a href="https://cs.gmu.edu/~amousavi/">Arsalan Mousavian</a> and <a href="https://homes.cs.washington.edu/~fox/">Dieter Fox</a></div>
                    <div class="booktitle">Proceedings of the IEEE International Conference on Robotics and Automation (ICRA). May 2021.</div>
                    <div>[<a href="#" title="Show Abstract" onclick="javascript:togglediv('eppner2020acronym_abstract'); return false;">Abstract</a>] [<a title="Show BibTex" href="#" onclick="javascript:togglediv('eppner2020acronym_bibtex'); return false;">BibTex</a>] [<a href="https://arxiv.org/pdf/2011.09584.pdf">PDF</a>]  [<a href="https://sites.google.com/nvidia.com/graspdataset">Data</a>] [<a href="https://github.com/NVlabs/acronym">Code &#9733;<small>47</small></a>]</div>
                    <div class="bibtex" id="eppner2020acronym_bibtex"><br />@inproceedings{eppner2020acronym,<br />&nbsp;&nbsp;title = {ACRONYM: A Large-Scale Grasp Dataset Based on Simulation},<br />&nbsp;&nbsp;author = {Eppner, Clemens and Mousavian, Arsalan and Fox, Dieter},<br />&nbsp;&nbsp;booktitle = {Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)},<br />&nbsp;&nbsp;year = {2021}<br />}</div>
                    <div class="abstract" id="eppner2020acronym_abstract">We introduce ACRONYM, a dataset for robot grasp planning based on physics simulation. The dataset contains 17.7M parallel-jaw grasps, spanning 8872 objects from 262 different categories, each labeled with the grasp result obtained from a physics simulator. We show the value of this large and diverse dataset by using it to train two state-of-the-art learning-based grasp planning algorithms. Grasp performance improves significantly when compared to the original smaller dataset.</div>
                </div>
            </div>
            
            <div class="publication">
                <div class="col-image"><img src="thumbnails/danielczuk2020rearrangement.png" width="100" alt="" /></div>
                <div class="col-rest">
                    <h5>Object Rearrangement Using Learned Implicit Collision Functions</h5>
                    <div class="authors"><a href="https://mjd3.github.io/">Michael Danielczuk</a>, <a href="https://cs.gmu.edu/~amousavi/">Arsalan Mousavian</a>, <b>Clemens Eppner</b> and <a href="https://homes.cs.washington.edu/~fox/">Dieter Fox</a></div>
                    <div class="booktitle">Proceedings of the IEEE International Conference on Robotics and Automation (ICRA). May 2021.</div>
                    <div>[<a href="#" title="Show Abstract" onclick="javascript:togglediv('danielczuk2020rearrangement_abstract'); return false;">Abstract</a>] [<a title="Show BibTex" href="#" onclick="javascript:togglediv('danielczuk2020rearrangement_bibtex'); return false;">BibTex</a>] [<a href="https://arxiv.org/pdf/2011.10726">PDF</a>]  [<a href="https://sites.google.com/nvidia.com/scenecollisionnet">Website</a>] [<a href="https://github.com/NVlabs/SceneCollisionNet">Code &#9733;<small>27</small></a>]</div>
                    <div class="bibtex" id="danielczuk2020rearrangement_bibtex"><br />@inproceedings{danielczuk2020rearrangement,<br />&nbsp;&nbsp;title = {Object Rearrangement Using Learned Implicit Collision Functions},<br />&nbsp;&nbsp;author = {Danielczuk, Michael and Mousavian, Arsalan and Eppner, Clemens and Fox, Dieter},<br />&nbsp;&nbsp;booktitle = {Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)},<br />&nbsp;&nbsp;year = {2021}<br />}</div>
                    <div class="abstract" id="danielczuk2020rearrangement_abstract">Robotic object rearrangement combines the skills of picking and placing objects. When object models are unavailable, typical collision-checking models may be unable to predict collisions in partial point clouds with occlusions, making generation of collision-free grasping or placement trajectories challenging. We propose a learned collision model that accepts scene and query object point clouds and predicts collisions for 6DOF object poses within the scene. We train the model on a synthetic set of 1 million scene/object point cloud pairs and 2 billion collision queries. We leverage the learned collision model as part of a model predictive path integral (MPPI) policy in a tabletop rearrangement task and show that the policy can plan collision-free grasps and placements for objects unseen in training in both simulated and physical cluttered scenes with a Franka Panda robot. The learned model outperforms both traditional pipelines and learned ablations by 9.8% in accuracy on a dataset of simulated collision queries and is 75x faster than the best-performing baseline.</div>
                </div>
            </div>
            
            <div class="publication">
                <div class="col-image"><img src="thumbnails/murali2019cluttergrasping.png" width="100" alt="" /></div>
                <div class="col-rest">
                    <h5>6-DOF Grasping for Target-driven Object Manipulation in Clutter</h5>
                    <div class="authors"><a href="http://adithyamurali.com/">Adithya Murali</a>, <a href="https://cs.gmu.edu/~amousavi/">Arsalan Mousavian</a>, <b>Clemens Eppner</b>, <a href="https://cpaxton.github.io/">Christopher Paxton</a> and <a href="https://homes.cs.washington.edu/~fox/">Dieter Fox</a></div>
                    <div class="booktitle">Proceedings of the IEEE International Conference on Robotics and Automation (ICRA). Paris, France. May 2020.</div>
                    <div>[<a href="#" title="Show Abstract" onclick="javascript:togglediv('murali2019cluttergrasping_abstract'); return false;">Abstract</a>] [<a title="Show BibTex" href="#" onclick="javascript:togglediv('murali2019cluttergrasping_bibtex'); return false;">BibTex</a>] [<a href="https://arxiv.org/pdf/1912.03628.pdf">PDF</a>]  [<a href="https://youtu.be/w0B5S-gCsJk">Video</a>] [<a href="https://youtu.be/nVpiMzIj0-c">Talk</a>]<br> [<a href="https://www.icra2020.org/program/conference-awards">ICRA 2020 Best Student & Best Manipulation Paper Award Finalist</a>]</div>
                    <div class="bibtex" id="murali2019cluttergrasping_bibtex"><br />@inproceedings{murali2019cluttergrasping,<br />&nbsp;&nbsp;title = {6-DOF Grasping for Target-driven Object Manipulation in Clutter},<br />&nbsp;&nbsp;author = {Murali, Adithya and Mousavian, Arsalan and Eppner, Clemens and Paxton, Christopher and Fox, Dieter},<br />&nbsp;&nbsp;booktitle = {Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)},<br />&nbsp;&nbsp;year = {2020}<br />}</div>
                    <div class="abstract" id="murali2019cluttergrasping_abstract">Grasping in cluttered environments is a fundamental but challenging robotic skill. It requires both reasoning about unseen object parts and potential collisions with the manipulator. Most existing data-driven approaches avoid this problem by limiting themselves to top-down planar grasps which is insufficient for many real-world scenarios and greatly limits possible grasps. We present a method that plans 6-DOF grasps for any desired object in a cluttered scene from partial point cloud observations. Our method achieves a grasp success of 80.3%, outperforming baseline approaches by 17.6% and clearing 9 cluttered table scenes (which contain 23 unknown objects and 51 picks in total) on a real robotic platform. By using our learned collision checking module, we can even reason about effective grasp sequences to retrieve objects that are not immediately accessible.</div>
                </div>
            </div>
            
            <div class="publication">
                <div class="col-image"><img src="thumbnails/deng2019selfsupervised.png" width="100" alt="" /></div>
                <div class="col-rest">
                    <h5>Self-supervised 6D Object Pose Estimation for Robot Manipulation</h5>
                    <div class="authors"><a href="https://scholar.google.com/citations?user=VIoAYzYAAAAJ&hl=en">Xinke Deng</a>, <a href="https://yuxng.github.io/">Yu Xiang</a>, <a href="https://cs.gmu.edu/~amousavi/">Arsalan Mousavian</a>, <b>Clemens Eppner</b>, <a href="https://aerospace.illinois.edu/directory/profile/tbretl">Timothy Bretl</a> and <a href="https://homes.cs.washington.edu/~fox/">Dieter Fox</a></div>
                    <div class="booktitle">Proceedings of the IEEE International Conference on Robotics and Automation (ICRA). Paris, France. May 2020.</div>
                    <div>[<a href="#" title="Show Abstract" onclick="javascript:togglediv('deng2019selfsupervised_abstract'); return false;">Abstract</a>] [<a title="Show BibTex" href="#" onclick="javascript:togglediv('deng2019selfsupervised_bibtex'); return false;">BibTex</a>] [<a href="https://arxiv.org/pdf/1909.10159">PDF</a>]  [<a href="https://youtu.be/W1Y0Mmh1Gd8">Video</a>]</div>
                    <div class="bibtex" id="deng2019selfsupervised_bibtex"><br />@inproceedings{deng2019selfsupervised,<br />&nbsp;&nbsp;title = {Self-supervised 6D Object Pose Estimation for Robot Manipulation},<br />&nbsp;&nbsp;author = {Deng, Xinke and Xiang, Yu and Mousavian, Arsalan and Eppner, Clemens and Bretl, Timothy and Fox, Dieter},<br />&nbsp;&nbsp;booktitle = {Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)},<br />&nbsp;&nbsp;year = {2020}<br />}</div>
                    <div class="abstract" id="deng2019selfsupervised_abstract">To teach robots skills, it is crucial to obtain data with supervision. Since annotating real world data is time-consuming and expensive, enabling robots to learn in a self-supervised way is important. In this work, we introduce a robot system for self-supervised 6D object pose estimation. Starting from modules trained in simulation, our system is able to label real world images with accurate 6D object poses for self-supervised learning. In addition, the robot interacts with objects in the environment to change the object configuration by grasping or pushing objects. In this way, our system is able to continuously collect data and improve its pose estimation modules. We show that the self-supervised learning improves object segmentation and 6D pose estimation performance, and consequently enables the system to grasp objects more reliably.</div>
                </div>
            </div>
            
            <div class="publication">
                <div class="col-image"><img src="thumbnails/paxton2019logicaldynamicalsystems.png" width="100" alt="" /></div>
                <div class="col-rest">
                    <h5>Representing Robot Task Plans as Robust Logical-Dynamical Systems</h5>
                    <div class="authors"><a href="https://cpaxton.github.io/">Christopher Paxton</a>, <a href="http://www.nathanratliff.com/">Nathan Ratliff</a>, <b>Clemens Eppner</b> and <a href="https://homes.cs.washington.edu/~fox/">Dieter Fox</a></div>
                    <div class="booktitle">Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). Macau, China. November 2019.</div>
                    <div>[<a href="#" title="Show Abstract" onclick="javascript:togglediv('paxton2019logicaldynamicalsystems_abstract'); return false;">Abstract</a>] [<a title="Show BibTex" href="#" onclick="javascript:togglediv('paxton2019logicaldynamicalsystems_bibtex'); return false;">BibTex</a>] [<a href="https://arxiv.org/pdf/1908.01896">PDF</a>] </div>
                    <div class="bibtex" id="paxton2019logicaldynamicalsystems_bibtex"><br />@inproceedings{paxton2019logicaldynamicalsystems,<br />&nbsp;&nbsp;title = {Representing Robot Task Plans as Robust Logical-Dynamical Systems},<br />&nbsp;&nbsp;author = {Christopher Paxton and Nathan Ratliff and Clemens Eppner and Dieter Fox},<br />&nbsp;&nbsp;booktitle = {Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},<br />&nbsp;&nbsp;year = {2019}<br />}</div>
                    <div class="abstract" id="paxton2019logicaldynamicalsystems_abstract">It is difficult to create robust, reusable, and reactive behaviors for robots that can be easily extended and combined. Frameworks such as Behavior Trees are flexible but difficult to characterize, especially when designing reactions and recovery behaviors to consistently converge to a desired goal condition. We propose a framework which we call Robust Logical-Dynamical Systems (RLDS), which combines the advantages of task representations like behavior trees with theoretical guarantees on performance. RLDS can also be constructed automatically from simple sequential task plans and will still achieve robust, reactive behavior in dynamic real-world environments. In this work, we describe both our proposed framework and a case study on a simple household manipulation task, with examples for how specific pieces can be implemented to achieve robust behavior. Finally, we show how in the context of these manipulation tasks, a combination of an RLDS with planning can achieve better results under adversarial conditions.</div>
                </div>
            </div>
            
            <div class="publication">
                <div class="col-image"><img src="thumbnails/eppner2019graspsampling.png" width="100" alt="" /></div>
                <div class="col-rest">
                    <h5>A Billion Ways to Grasp: An Evaluation of Grasp Sampling Schemes on a Dense, Physics-Based Grasp Data Set</h5>
                    <div class="authors"><b>Clemens Eppner</b>, <a href="https://cs.gmu.edu/~amousavi/">Arsalan Mousavian</a> and <a href="https://homes.cs.washington.edu/~fox/">Dieter Fox</a></div>
                    <div class="booktitle">Springer Proceedings of the 19th International Symposium of Robotics Research (ISRR). Hanoi, Vietnam. October 2019.</div>
                    <div>[<a href="#" title="Show Abstract" onclick="javascript:togglediv('eppner2019graspsampling_abstract'); return false;">Abstract</a>] [<a title="Show BibTex" href="#" onclick="javascript:togglediv('eppner2019graspsampling_bibtex'); return false;">BibTex</a>] [<a href="https://arxiv.org/pdf/1912.05604">PDF</a>]  [<a href="https://sites.google.com/view/abillionwaystograsp/">Website</a>]</div>
                    <div class="bibtex" id="eppner2019graspsampling_bibtex"><br />@inproceedings{eppner2019graspsampling,<br />&nbsp;&nbsp;title = {A Billion Ways to Grasp: An Evaluation of Grasp Sampling Schemes on a Dense, Physics-Based Grasp Data Set},<br />&nbsp;&nbsp;author = {Clemens Eppner and Arsalan Mousavian and Dieter Fox},<br />&nbsp;&nbsp;booktitle = {Springer Proceedings of the 19th International Symposium of Robotics Research (ISRR)},<br />&nbsp;&nbsp;year = {2019}<br />}</div>
                    <div class="abstract" id="eppner2019graspsampling_abstract">Robot grasping is often formulated as a learning problem. With the increasing speed and quality of physics simulations, generating large-scale grasping data sets that feed learning algorithms is becoming more and more popular. An often overlooked question is how to generate the grasps that make up these data sets. In this paper, we review, classify, and compare different grasp sampling strategies. Our evaluation is based on a fine-grained discretization of SE(3) and uses physics-based simulation to evaluate the quality and robustness of the corresponding parallel-jaw grasps. Specifically, we consider more than 1 billion grasps for each of the 21 objects from the YCB data set. This dense data set lets us evaluate existing sampling schemes w.r.t. their bias and efficiency. Our experiments show that some popular sampling schemes contain significant bias and do not cover all possible ways an object can be grasped.</div>
                </div>
            </div>
            
            <div class="publication">
                <div class="col-image"><img src="thumbnails/mousavian2019graspnet.png" width="100" alt="" /></div>
                <div class="col-rest">
                    <h5>6-DOF GraspNet: Variational Grasp Generation for Object Manipulation</h5>
                    <div class="authors"><a href="https://cs.gmu.edu/~amousavi/">Arsalan Mousavian</a>, <b>Clemens Eppner</b> and <a href="https://homes.cs.washington.edu/~fox/">Dieter Fox</a></div>
                    <div class="booktitle">Proceedings of the International Conference on Computer Vision (ICCV). Seoul, Korea. October 2019.</div>
                    <div>[<a href="#" title="Show Abstract" onclick="javascript:togglediv('mousavian2019graspnet_abstract'); return false;">Abstract</a>] [<a title="Show BibTex" href="#" onclick="javascript:togglediv('mousavian2019graspnet_bibtex'); return false;">BibTex</a>] [<a href="https://arxiv.org/pdf/1905.10520">PDF</a>]  [<a href="https://www.youtube.com/watch?v=y5EJXeEiB1o">Video</a>] [<a href="https://research.nvidia.com/publication/2019-10_6-DOF-GraspNet%3A-Variational">Website</a>] [<a href="https://github.com/NVlabs/6dof-graspnet">Code &#9733;<small>137</small></a>] [<span class="link_color">Press:</span> <a href="https://www.neowin.net/news/nvidias-new-algorithm6-dof-graspnethelps-robots-pick-up-arbitrary-objects/">Neowin</a>]</div>
                    <div class="bibtex" id="mousavian2019graspnet_bibtex"><br />@inproceedings{mousavian2019graspnet,<br />&nbsp;&nbsp;title = {6-DOF GraspNet: Variational Grasp Generation for Object Manipulation},<br />&nbsp;&nbsp;author = {Arsalan Mousavian and Clemens Eppner and Dieter Fox},<br />&nbsp;&nbsp;booktitle = {Proceedings of the International Conference on Computer Vision (ICCV)},<br />&nbsp;&nbsp;year = {2019}<br />}</div>
                    <div class="abstract" id="mousavian2019graspnet_abstract">Generating grasp poses is a crucial component for any robot object manipulation task. In this work, we formulate the problem of grasp generation as sampling a set of grasps using a variational autoencoder and assess and refine the sampled grasps using a grasp evaluator model. Both Grasp Sampler and Grasp Refinement networks take 3D point clouds observed by a depth camera as input. We evaluate our approach in simulation and real-world robot experiments. Our approach achieves 88% success rate on various commonly used objects with diverse appearances, scales, and weights. Our model is trained purely in simulation and works in the real world without any extra steps.</div>
                </div>
            </div>
            
            <div class="publication">
                <div class="col-image"><img src="thumbnails/martin2018dataset.png" width="100" alt="" /></div>
                <div class="col-rest">
                    <h5>The RBO Dataset of Articulated Objects and Interactions</h5>
                    <div class="authors"><a href="https://robertomartinmartin.com/">Roberto Martín-Martín</a>*, <b>Clemens Eppner</b>* and <a href="https://www.robotics.tu-berlin.de/menue/team/oliver_brock/">Oliver Brock</a></div>
                    <div class="booktitle">The International Journal of Robotics Research. SAGE Publications. August 2019.</div>
                    <div>[<a href="#" title="Show Abstract" onclick="javascript:togglediv('martin2018dataset_abstract'); return false;">Abstract</a>] [<a title="Show BibTex" href="#" onclick="javascript:togglediv('martin2018dataset_bibtex'); return false;">BibTex</a>] [<a href="https://arxiv.org/pdf/1806.06465">PDF</a>]  [<a href="https://tu-rbo.github.io/articulated-objects/">Data</a>]</div>
                    <div class="bibtex" id="martin2018dataset_bibtex"><br />@article{martin2018dataset,<br />&nbsp;&nbsp;title = {The RBO Dataset of Articulated Objects and Interactions},<br />&nbsp;&nbsp;author = {Roberto Mart\'{\i}n-Mart\'{\i}n and Clemens Eppner and Brock, Oliver},<br />&nbsp;&nbsp;journal = {The International Journal of Robotics Research},<br />&nbsp;&nbsp;year = {2019}<br />}</div>
                    <div class="abstract" id="martin2018dataset_abstract">We present a dataset with models of 14 articulated objects commonly found in human environments and with RGB-D video sequences and wrenches recorded of human interactions with them. The 358 interaction sequences total 67 minutes of human manipulation under varying experimental conditions (type of interaction, lighting, perspective, and background). Each interaction with an object is annotated with the ground-truth poses of its rigid parts and the kinematic state obtained by a motion-capture system. For a subset of 78 sequences (25 minutes), we also measured the interaction wrenches. The object models contain textured three-dimensional triangle meshes of each link and their motion constraints. We provide Python scripts to download and visualize the data. The data are available at https://turbo.github.io/articulated-objects/ and hosted at https://zenodo.org/record/1036660/.</div>
                </div>
            </div>
            
            <div class="publication">
                <div class="col-image"><img src="thumbnails/eppner2018physics.png" width="100" alt="" /></div>
                <div class="col-rest">
                    <h5>Physics-Based Selection of Informative Actions for Interactive Perception</h5>
                    <div class="authors"><b>Clemens Eppner</b>*, <a href="https://robertomartinmartin.com/">Roberto Martín-Martín</a>* and <a href="https://www.robotics.tu-berlin.de/menue/team/oliver_brock/">Oliver Brock</a></div>
                    <div class="booktitle">Proceedings of the IEEE International Conference on Robotics and Automation (ICRA). Brisbane, Australia. May 2018.</div>
                    <div>[<a href="#" title="Show Abstract" onclick="javascript:togglediv('eppner2018physics_abstract'); return false;">Abstract</a>] [<a title="Show BibTex" href="#" onclick="javascript:togglediv('eppner2018physics_bibtex'); return false;">BibTex</a>] [<a href="https://www.robotics.tu-berlin.de/fileadmin/fg170/Publikationen_pdf/eppnermartin_18_icra.pdf">PDF</a>]  [<a href="https://www.youtube.com/watch?v=w8FgpwdIZ2g">Video</a>]</div>
                    <div class="bibtex" id="eppner2018physics_bibtex"><br />@inproceedings{eppner2018physics,<br />&nbsp;&nbsp;title = {Physics-Based Selection of Informative Actions for Interactive Perception},<br />&nbsp;&nbsp;author = {Clemens Eppner and Roberto Mart\'{\i}n-Mart\'{\i}n and Oliver Brock},<br />&nbsp;&nbsp;booktitle = {Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)},<br />&nbsp;&nbsp;year = {2018}<br />}</div>
                    <div class="abstract" id="eppner2018physics_abstract">Interactive perception exploits the correlation between forceful interactions and changes in the observed signals to extract task-relevant information from the sensor stream. Finding the most informative interactions to perceive complex objects, like articulated mechanisms, is challenging because the outcome of the interaction is difficult to predict. We propose a method to select the most informative action while deriving a model of articulated mechanisms that includes kinematic, geometric, and dynamic properties. Our method addresses the complexity of the action selection task based on two insights. First, we show that for a class of interactive perception methods, information gain can be approximated by the amount of motion induced in the mechanism. Second, we resort to physics simulations grounded in the real-world through interactive perception to predict possible action outcomes. Our method enables the robot to autonomously select actions for interactive perception that reveal most information, given the current knowledge ofthe world. This leads to improved perception and more accurate world models, finally enabling robust manipulation.</div>
                </div>
            </div>
            
            <div class="publication">
                <div class="col-image"><img src="thumbnails/eppner2017visual.png" width="100" alt="" /></div>
                <div class="col-rest">
                    <h5>Visual Detection of Opportunities to Exploit Contact in Grasping Using Contextual Multi-Armed Bandits</h5>
                    <div class="authors"><b>Clemens Eppner</b> and <a href="https://www.robotics.tu-berlin.de/menue/team/oliver_brock/">Oliver Brock</a></div>
                    <div class="booktitle">Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). Vancouver, Canada. September 2017.</div>
                    <div>[<a href="#" title="Show Abstract" onclick="javascript:togglediv('eppner2017visual_abstract'); return false;">Abstract</a>] [<a title="Show BibTex" href="#" onclick="javascript:togglediv('eppner2017visual_bibtex'); return false;">BibTex</a>] [<a href="http://www.robotics.tu-berlin.de/fileadmin/fg170/Publikationen_pdf/Eppner17_IROS.pdf">PDF</a>]  [<a href="https://www.youtube.com/watch?v=xFw-zetsMfY">Video</a>]</div>
                    <div class="bibtex" id="eppner2017visual_bibtex"><br />@inproceedings{eppner2017visual,<br />&nbsp;&nbsp;title = {Visual Detection of Opportunities to Exploit Contact in Grasping Using Contextual Multi-Armed Bandits},<br />&nbsp;&nbsp;author = {Clemens Eppner and Oliver Brock},<br />&nbsp;&nbsp;booktitle = {Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},<br />&nbsp;&nbsp;year = {2017}<br />}</div>
                    <div class="abstract" id="eppner2017visual_abstract">Environment-constrained grasping exploits beneficial interactions between hand, object, and environment to increase grasp success. Instead of focusing on the final static relationship between hand posture and object pose, this view of grasping emphasizes the need and the opportunity to select the most appropriate, contact-rich grasping motion, leading up to a final static grasp configuration. This view changes the nature of the underlying planning problem: Instead of planning for static contact points, we need to decide which environmental constraint (EC) to use during the grasping motion. We propose a method to make these decisions based on depth measurements so as to generate robust grasps for a large variety of objects. Our planner exploits the advantages of a soft robot  hand and learns a hand-specific classifier for edge-, surface-, and wall-grasps, each exploiting a different EC. Additionally, we show how the model can continuously be improved in a contextual multi-armed bandit setting without an explicit training and test phase, enabling the continuous improvement of a robot’s grasping skills throughout life time.</div>
                </div>
            </div>
            
            <div class="publication">
                <div class="col-image"><img src="thumbnails/sieverling2017interleaving.png" width="100" alt="" /></div>
                <div class="col-rest">
                    <h5>Interleaving Motion in Contact and in Free Space for Planning Under Uncertainty</h5>
                    <div class="authors"><a href="https://scholar.google.com/citations?user=U9g1pEQAAAAJ&hl=en">Arne Sieverling</a>, <b>Clemens Eppner</b>, <a href="https://scholar.google.com/citations?hl=en&user=kV4iUI4AAAAJ">Felix Wolff</a> and <a href="https://www.robotics.tu-berlin.de/menue/team/oliver_brock/">Oliver Brock</a></div>
                    <div class="booktitle">Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). Vancouver, Canada. September 2017.</div>
                    <div>[<a href="#" title="Show Abstract" onclick="javascript:togglediv('sieverling2017interleaving_abstract'); return false;">Abstract</a>] [<a title="Show BibTex" href="#" onclick="javascript:togglediv('sieverling2017interleaving_bibtex'); return false;">BibTex</a>] [<a href="http://www.robotics.tu-berlin.de/fileadmin/fg170/Publikationen_pdf/Sieverling17_IROS.pdf">PDF</a>]  [<a href="https://www.youtube.com/watch?v=CXaN8ZWRMT0">Video</a>]</div>
                    <div class="bibtex" id="sieverling2017interleaving_bibtex"><br />@inproceedings{sieverling2017interleaving,<br />&nbsp;&nbsp;title = {Interleaving Motion in Contact and in Free Space for Planning Under Uncertainty},<br />&nbsp;&nbsp;author = {Arne Sieverling and Clemens Eppner and Felix Wolff and Oliver Brock},<br />&nbsp;&nbsp;booktitle = {Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},<br />&nbsp;&nbsp;year = {2017}<br />}</div>
                    <div class="abstract" id="sieverling2017interleaving_abstract">In this paper we present a planner that interleaves free-space motion with motion in contact to reduce uncertainty. The planner finds such motions by growing a search tree in the combined space of collision-free and contact configurations. The planner reasons  efficiently about the accumulated uncertainty by factoring the state in a belief over configuration and a fully observable contact state. We show the uncertainty-reducing capabilities of the planner on manipulation benchmark from the POMDP literature. The planner scales up to more complex problems like manipulation under uncertainty in seven-dimensional configuration space. We validate our planner in simulation and on a real robot.</div>
                </div>
            </div>
            
            <div class="publication">
                <div class="col-image"><img src="thumbnails/gupta2016learning.png" width="100" alt="" /></div>
                <div class="col-rest">
                    <h5>Learning Dexterous Manipulation for a Soft Robotic Hand from Human Demonstrations</h5>
                    <div class="authors"><a href="https://people.eecs.berkeley.edu/~abhigupta/">Abhishek Gupta</a>, <b>Clemens Eppner</b>, <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a> and <a href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a></div>
                    <div class="booktitle">Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). Deajeon, South Korea. October 2016.</div>
                    <div>[<a href="#" title="Show Abstract" onclick="javascript:togglediv('gupta2016learning_abstract'); return false;">Abstract</a>] [<a title="Show BibTex" href="#" onclick="javascript:togglediv('gupta2016learning_bibtex'); return false;">BibTex</a>] [<a href="https://arxiv.org/pdf/1603.06348">PDF</a>]  [<a href="https://www.youtube.com/watch?v=XyZFkJWu0Q0">Video</a>]</div>
                    <div class="bibtex" id="gupta2016learning_bibtex"><br />@inproceedings{gupta2016learning,<br />&nbsp;&nbsp;title = {Learning Dexterous Manipulation for a Soft Robotic Hand from Human Demonstrations},<br />&nbsp;&nbsp;author = {Gupta, Abhishek and Eppner, Clemens and Levine, Sergey and Abbeel, Pieter},<br />&nbsp;&nbsp;booktitle = {Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},<br />&nbsp;&nbsp;year = {2016}<br />}</div>
                    <div class="abstract" id="gupta2016learning_abstract">Dexterous multi-fingered hands can accomplish fine manipulation behaviors that are infeasible with simple robotic grippers. However, sophisticated multi-fingered hands are often expensive and fragile. Low-cost soft hands offer an appealing alternative to more conventional devices, but present considerable challenges in sensing and actuation, making them difficult to apply to more complex manipulation tasks. In this paper, we describe an approach to learning from demonstration that can be used to train soft robotic hands to perform dexterous manipulation tasks. Our method uses object-centric demonstrations, where a human demonstrates the desired motion of manipulated objects with their own hands, and the robot autonomously learns to imitate these demonstrations using reinforcement learning. We propose a novel algorithm that allows us to blend and select a subset of the most feasible demonstrations to learn to imitate on the hardware, which we use with an extension of the guided policy search framework to use multiple demonstrations to learn generalizable neural network policies. We demonstrate our approach on the RBO Hand 2, with learned motor skills for turning a valve, manipulating an abacus, and grasping.</div>
                </div>
            </div>
            
            <div class="publication">
                <div class="col-image"><img src="thumbnails/jonschkowski2016probabilistic.png" width="100" alt="" /></div>
                <div class="col-rest">
                    <h5>Probabilistic Multi-Class Segmentation for the Amazon Picking Challenge</h5>
                    <div class="authors"><a href="http://ricojonschkowski.com/">Rico Jonschkowski</a>, <b>Clemens Eppner</b>*, <a href="https://shoefer.github.io/me/">Sebastian Höfer</a>*, <a href="https://robertomartinmartin.com/">Roberto Martín-Martín</a>* and <a href="https://www.robotics.tu-berlin.de/menue/team/oliver_brock/">Oliver Brock</a></div>
                    <div class="booktitle">Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). Deajeon, South Korea. October 2016.</div>
                    <div>[<a href="#" title="Show Abstract" onclick="javascript:togglediv('jonschkowski2016probabilistic_abstract'); return false;">Abstract</a>] [<a title="Show BibTex" href="#" onclick="javascript:togglediv('jonschkowski2016probabilistic_bibtex'); return false;">BibTex</a>] [<a href="http://www.robotics.tu-berlin.de/fileadmin/fg170/Publikationen_pdf/Jonschkowski-16-IROS.pdf">PDF</a>]  [<a href="https://www.youtube.com/watch?v=Ry6JzeW0HOM">Video</a>]<br> [<a href="http://www.iros2016.org/awards.html">IROS 2016 Best Paper Award Finalist</a>]</div>
                    <div class="bibtex" id="jonschkowski2016probabilistic_bibtex"><br />@inproceedings{jonschkowski2016probabilistic,<br />&nbsp;&nbsp;title = {Probabilistic Multi-Class Segmentation for the Amazon Picking Challenge},<br />&nbsp;&nbsp;author = {Rico Jonschkowski and Clemens Eppner and Sebastian H{\"o}fer and Roberto Mart\'{\i}n-Mart\'{\i}n and Oliver Brock },<br />&nbsp;&nbsp;booktitle = {Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},<br />&nbsp;&nbsp;year = {2016}<br />}</div>
                    <div class="abstract" id="jonschkowski2016probabilistic_abstract">We present a method for multi-class segmentation from RGB-D data in a realistic warehouse picking setting. The method computes pixel-wise probabilities and combines them to find a coherent object segmentation. It reliably segments objects in cluttered scenarios, even when objects are translucent, reflective, highly deformable, have fuzzy surfaces, or consist of loosely coupled components. The robust performance results from the exploitation of problem structure inherent to the warehouse setting. The proposed method proved its capabilities as part of our winning entry to the 2015 Amazon Picking Challenge. We present a detailed experimental analysis of the contribution of different information sources, compare our method to standard segmentation techniques, and assess possible extensions that further enhance the algorithm's capabilities. We release our software and data sets as open source.</div>
                </div>
            </div>
            
            <div class="publication">
                <div class="col-image"><img src="thumbnails/eppner2016lessons.png" width="100" alt="" /></div>
                <div class="col-rest">
                    <h5>Lessons from the Amazon Picking Challenge: Four Aspects of Building Robotic Systems</h5>
                    <div class="authors"><b>Clemens Eppner</b>*, <a href="https://shoefer.github.io/me/">Sebastian Höfer</a>*, <a href="http://ricojonschkowski.com/">Rico Jonschkowski</a>*, <a href="https://robertomartinmartin.com/">Roberto Martín-Martín</a>*, <a href="https://scholar.google.com/citations?user=U9g1pEQAAAAJ&hl=en">Arne Sieverling</a>*, <a href="https://scholar.google.com/citations?user=8wnaBccAAAAJ&hl=de">Vincent Wall</a>* and <a href="https://www.robotics.tu-berlin.de/menue/team/oliver_brock/">Oliver Brock</a></div>
                    <div class="booktitle">Proceedings of Robotics: Science and Systems (RSS). Ann Arbor, Michigan, USA. June 2016.</div>
                    <div>[<a href="#" title="Show Abstract" onclick="javascript:togglediv('eppner2016lessons_abstract'); return false;">Abstract</a>] [<a title="Show BibTex" href="#" onclick="javascript:togglediv('eppner2016lessons_bibtex'); return false;">BibTex</a>] [<a href="http://www.redaktion.tu-berlin.de/fileadmin/fg170/Publikationen_pdf/apc_rbo_rss2016_final.pdf">PDF</a>]  [<a href="https://www.youtube.com/watch?v=UrpMfdj-Mpc">Video</a>] [<span class="link_color">Press:</span> <a href="http://www.engadget.com/2015/06/01/amazon-picking-challenge-winner/">Engadget</a><span class="link_color">,</span> <a href="http://robohub.org/team-rbo-from-berlin-wins-amazon-picking-challenge-convincingly/">RoboHub</a>]<br> [<a href="http://rss2016.engin.umich.edu/awards.html#systemspaper">RSS 2016 Best Systems Paper Award</a>]</div>
                    <div class="bibtex" id="eppner2016lessons_bibtex"><br />@inproceedings{eppner2016lessons,<br />&nbsp;&nbsp;title = {Lessons from the Amazon Picking Challenge: Four Aspects of Building Robotic Systems},<br />&nbsp;&nbsp;author = {Clemens Eppner and Sebastian H{\"o}fer and Rico Jonschkowski and Roberto Mart\'{\i}n-Mart\'{\i}n and Arne Sieverling and Vincent Wall and Oliver Brock},<br />&nbsp;&nbsp;booktitle = {Proceedings of Robotics: Science and Systems (RSS)},<br />&nbsp;&nbsp;year = {2016}<br />}</div>
                    <div class="abstract" id="eppner2016lessons_abstract">We describe the winning entry to the Amazon Picking Challenge. From the experience of building this system and competing in the Amazon Picking Challenge, we derive several conclusions: 1) We suggest to characterize robotic systems building along four key aspects, each of them spanning a spectrum of solutions: modularity vs. integration, generality vs. assumptions, computation vs. embodiment, and planning vs. feedback. 2) To understand which region of each spectrum most adequately addresses which robotic problem, we must explore the full spectrum of possible approaches. To achieve this, our community should agree on key aspects that characterize the solution space of robotic systems. 3) For manipulation problems in unstructured environments, certain regions of each spectrum match the problem most adequately, and should be exploited further. This is supported by the fact that our solution deviated from the majority of the other challenge entries along each of the spectra.</div>
                </div>
            </div>
            
            <div class="publication">
                <div class="col-image"><img src="thumbnails/mordatch2016combining.png" width="100" alt="" /></div>
                <div class="col-rest">
                    <h5>Combining Model-Based Policy Search with Online Model Learning for Control of Physical Humanoids</h5>
                    <div class="authors"><a href="https://scholar.google.com/citations?hl=de&user=Vzr1RukAAAAJ">Igor Mordatch</a>, <a href="https://nikhilmishra000.github.io/">Nikhil Mishra</a>, <b>Clemens Eppner</b> and <a href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a></div>
                    <div class="booktitle">Proceedings of the IEEE International Conference on Robotics and Automation (ICRA). Stockholm, Sweden. May 2016.</div>
                    <div>[<a href="#" title="Show Abstract" onclick="javascript:togglediv('mordatch2016combining_abstract'); return false;">Abstract</a>] [<a title="Show BibTex" href="#" onclick="javascript:togglediv('mordatch2016combining_bibtex'); return false;">BibTex</a>] [<a href="http://www.robotics.tu-berlin.de/fileadmin/fg170/Publikationen_pdf/mordatch2016.pdf">PDF</a>]  [<span class="link_color">Press:</span> <a href="https://www.technologyreview.com/s/542921/robot-toddler-learns-to-stand-by-imagining-how-to-do-it/">MIT Technology Review</a>]</div>
                    <div class="bibtex" id="mordatch2016combining_bibtex"><br />@conference{mordatch2016combining,<br />&nbsp;&nbsp;title = {Combining Model-Based Policy Search with Online Model Learning for Control of Physical Humanoids},<br />&nbsp;&nbsp;author = {Igor Mordatch and Nikhil Mishra and Clemens Eppner and Pieter Abbeel},<br />&nbsp;&nbsp;booktitle = {Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)},<br />&nbsp;&nbsp;year = {2016}<br />}</div>
                    <div class="abstract" id="mordatch2016combining_abstract">We present an automatic method for interactive control of physical humanoid robots based on high-level tasks that does not require manual specification of motion trajectories or specially-designed control policies. The method is based on the combination of a model-based policy that is trained off-line in simulation and sends high-level commands to a model-free controller that executes these commands on the physical robot. This low-level controller simultaneously learns and adapts a local model of dynamics on-line and computes optimal controls under the learned model. The high-level policy is trained using a combination of trajectory optimization and neural network learning, while considering physical limitations such as limited sensors and communication delays. The entire system runs in real-time on the robot's computer and uses only on-board sensors. We demonstrate successful policy execution on a range of tasks such as leaning, hand reaching, and robust balancing behaviors atop a tilting base on the physical robot and in simulation.</div>
                </div>
            </div>
            
            <div class="publication">
                <div class="col-image"><img src="thumbnails/eppner2015planning.png" width="100" alt="" /></div>
                <div class="col-rest">
                    <h5>Planning Grasp Strategies That Exploit Environmental Constraints</h5>
                    <div class="authors"><b>Clemens Eppner</b> and <a href="https://www.robotics.tu-berlin.de/menue/team/oliver_brock/">Oliver Brock</a></div>
                    <div class="booktitle">Proceedings of the IEEE International Conference on Robotics and Automation (ICRA). Seattle, USA. May 2015.</div>
                    <div>[<a href="#" title="Show Abstract" onclick="javascript:togglediv('eppner2015planning_abstract'); return false;">Abstract</a>] [<a title="Show BibTex" href="#" onclick="javascript:togglediv('eppner2015planning_bibtex'); return false;">BibTex</a>] [<a href="http://www.robotics.tu-berlin.de/fileadmin/fg170/Publikationen_pdf/eppner_icra2015.pdf">PDF</a>]  [<a href="https://www.youtube.com/watch?v=Va0O0JS9bx0">Video</a>] [<a href="https://github.com/SoMa-Project/ec_grasp_planner">Code &#9733;<small>7</small></a>]</div>
                    <div class="bibtex" id="eppner2015planning_bibtex"><br />@inproceedings{eppner2015planning,<br />&nbsp;&nbsp;title = {Planning Grasp Strategies That Exploit Environmental Constraints},<br />&nbsp;&nbsp;author = {Clemens Eppner and Oliver Brock},<br />&nbsp;&nbsp;booktitle = {Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)},<br />&nbsp;&nbsp;year = {2015}<br />}</div>
                    <div class="abstract" id="eppner2015planning_abstract">There is strong evidence that robustness in human and robotic grasping can be achieved through the deliberate exploitation of contact with the environment. In contrast to this, traditional grasp planners generally disregard the opportunity to interact with the environment during grasping. In this paper, we propose a novel view of grasp planning that centers on the exploitation of environmental contact. In this view, grasps are sequences of constraint exploitations, i.e. consecutive motions constrained by features in the environment, ending in a grasp. To be able to generate such grasp plans, it becomes necessary to consider planning, perception, and control as tightly integrated components. As a result, each of these components can be simplified while still yielding reliable grasping performance. We propose a first implementation of a grasp planner based on this view and demonstrate in real-world experiments the robustness and versatility of the resulting grasp plans.</div>
                </div>
            </div>
            
            <div class="publication">
                <div class="col-image"><img src="thumbnails/heinemann2015taxonomy.png" width="100" alt="" /></div>
                <div class="col-rest">
                    <h5>A Taxonomy of Human Grasping Behavior Suitable for Transfer to Robotic Hands</h5>
                    <div class="authors"><a href="">Fabian Heinemann</a>, <a href="https://www.robotics.tu-berlin.de/menue/team/steffen_puhlmann/">Steffen Puhlmann</a>, <b>Clemens Eppner</b>, <a href="https://www.mi.fu-berlin.de/inf/groups/ag-ki/members/Scientific-Staff/Alvarez.html">José Álvarez-Ruiz</a>, <a href="https://www.psyco.tu-berlin.de/maertens.html">Marianne Maertens</a> and <a href="https://www.robotics.tu-berlin.de/menue/team/oliver_brock/">Oliver Brock</a></div>
                    <div class="booktitle">Proceedings of the IEEE International Conference on Robotics and Automation (ICRA). Seattle, USA. May 2015.</div>
                    <div>[<a href="#" title="Show Abstract" onclick="javascript:togglediv('heinemann2015taxonomy_abstract'); return false;">Abstract</a>] [<a title="Show BibTex" href="#" onclick="javascript:togglediv('heinemann2015taxonomy_bibtex'); return false;">BibTex</a>] [<a href="http://www.robotics.tu-berlin.de/fileadmin/fg170/Publikationen_pdf/heinemann_puhlmann_icra2015.pdf">PDF</a>] </div>
                    <div class="bibtex" id="heinemann2015taxonomy_bibtex"><br />@inproceedings{heinemann2015taxonomy,<br />&nbsp;&nbsp;title = {A Taxonomy of Human Grasping Behavior Suitable for Transfer to Robotic Hands},<br />&nbsp;&nbsp;author = {Heinemann, Fabian and Puhlmann, Steffen and Eppner, Clemens and {\'A}lvarez-Ruiz, Jos{\'e} and Maertens, Marianne and Brock, Oliver},<br />&nbsp;&nbsp;booktitle = {Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)},<br />&nbsp;&nbsp;year = {2015}<br />}</div>
                    <div class="abstract" id="heinemann2015taxonomy_abstract">As a first step towards transferring human grasping capabilities to robots, we analyzed the grasping behavior of human subjects. We derived a taxonomy in order to adequately represent the observed strategies. During the analysis of the recorded data, this classification scheme helped us to obtain a better understanding of human grasping behavior. We will provide support for our hypothesis that humans exploit compliant contact between the hand and the environment to compensate for uncertainty. We will also show a realization of the resulting grasping strategies on a real robot. It is our belief that the detailed analysis of human grasping behavior will ultimately lead to significant increases in robot manipulation and dexterity.</div>
                </div>
            </div>
            
            <div class="publication">
                <div class="col-image"><img src="thumbnails/eppner2015exploitation.png" width="100" alt="" /></div>
                <div class="col-rest">
                    <h5>Exploitation of Environmental Constraints in Human and Robotic Grasping</h5>
                    <div class="authors"><b>Clemens Eppner</b>, <a href="https://www.user.tu-berlin.de//raphael.deimel/researchprofile.html">Raphael Deimel</a>, <a href="https://www.mi.fu-berlin.de/inf/groups/ag-ki/members/Scientific-Staff/Alvarez.html">José Álvarez-Ruiz</a>, <a href="https://www.psyco.tu-berlin.de/maertens.html">Marianne Maertens</a> and <a href="https://www.robotics.tu-berlin.de/menue/team/oliver_brock/">Oliver Brock</a></div>
                    <div class="booktitle">The International Journal of Robotics Research. SAGE Publications. April 2015.</div>
                    <div>[<a href="#" title="Show Abstract" onclick="javascript:togglediv('eppner2015exploitation_abstract'); return false;">Abstract</a>] [<a title="Show BibTex" href="#" onclick="javascript:togglediv('eppner2015exploitation_bibtex'); return false;">BibTex</a>] [<a href="http://www.robotics.tu-berlin.de/fileadmin/fg170/Publikationen_pdf/Eppner-Deimel-Ruiz-2015-IJRR.pdf">PDF</a>]  [<a href="https://www.youtube.com/watch?v=ZOqY_ZBrKqA">Video</a>]</div>
                    <div class="bibtex" id="eppner2015exploitation_bibtex"><br />@article{eppner2015exploitation,<br />&nbsp;&nbsp;title = {Exploitation of Environmental Constraints in Human and Robotic Grasping},<br />&nbsp;&nbsp;author = {Eppner, Clemens and Deimel, Raphael and {\'A}lvarez-Ruiz, Jos{\'e} and Maertens, Marianne and Brock, Oliver},<br />&nbsp;&nbsp;journal = {The International Journal of Robotics Research},<br />&nbsp;&nbsp;year = {2015}<br />}</div>
                    <div class="abstract" id="eppner2015exploitation_abstract">We investigate the premise that robust grasping performance is enabled by exploiting constraints present in the environment. These constraints, leveraged through motion in contact, counteract uncertainty in state variables relevant to grasp success. Given this premise, grasping becomes a process of successive exploitation of environmental constraints, until a successful grasp has been established. We present support for this view found through the analysis of human grasp behavior and by showing robust robotic grasping based on constraint-exploiting grasp strategies. Furthermore, we show that it is possible to design robotic hands with inherent capabilities for the exploitation of environmental constraints.</div>
                </div>
            </div>
            
            <div class="publication">
                <div class="col-image"><img src="thumbnails/deimel2013exploitation.png" width="100" alt="" /></div>
                <div class="col-rest">
                    <h5>Exploitation of Environmental Constraints in Human and Robotic Grasping</h5>
                    <div class="authors"><a href="https://www.user.tu-berlin.de//raphael.deimel/researchprofile.html">Raphael Deimel</a>, <b>Clemens Eppner</b>, <a href="https://www.mi.fu-berlin.de/inf/groups/ag-ki/members/Scientific-Staff/Alvarez.html">José Álvarez-Ruiz</a>, <a href="https://www.psyco.tu-berlin.de/maertens.html">Marianne Maertens</a> and <a href="https://www.robotics.tu-berlin.de/menue/team/oliver_brock/">Oliver Brock</a></div>
                    <div class="booktitle">Proceedings of the 16th International Symposium on Robotics Research (ISRR). Singapore. December 2013.</div>
                    <div>[<a href="#" title="Show Abstract" onclick="javascript:togglediv('deimel2013exploitation_abstract'); return false;">Abstract</a>] [<a title="Show BibTex" href="#" onclick="javascript:togglediv('deimel2013exploitation_bibtex'); return false;">BibTex</a>] [<a href="http://www.robotics.tu-berlin.de/fileadmin/fg170/Publikationen_pdf/isrr2013.pdf">PDF</a>] </div>
                    <div class="bibtex" id="deimel2013exploitation_bibtex"><br />@inproceedings{deimel2013exploitation,<br />&nbsp;&nbsp;title = {Exploitation of Environmental Constraints in Human and Robotic Grasping},<br />&nbsp;&nbsp;author = {Raphael Deimel and Clemens Eppner and Jos{\'e} {\'A}lvarez-Ruiz and Marianne Maertens and Oliver Brock},<br />&nbsp;&nbsp;booktitle = {Proceedings of the 16th International Symposium on Robotics Research (ISRR)},<br />&nbsp;&nbsp;year = {2013}<br />}</div>
                    <div class="abstract" id="deimel2013exploitation_abstract">We investigate the premise that robust grasping performance is enabled by exploiting constraints present in the environment. These constraints, leveraged through motion in contact, counteract uncertainty in state variables relevant to grasp success. Given this premise, grasping becomes a process of successive exploitation of environmental constraints, until a successful grasp has been established. We present support for this view by analyzing human grasp behavior and by showing robust robotic grasping based on constraint-exploiting grasp strategies. Furthermore, we show that it is possible to design robotic hands with inherent capabilities for the exploitation of environmental constraints.</div>
                </div>
            </div>
            
            <div class="publication">
                <div class="col-image"><img src="thumbnails/eppner2013grasping.png" width="100" alt="" /></div>
                <div class="col-rest">
                    <h5>Grasping Unknown Objects by Exploiting Shape Adaptability and Environmental Constraints</h5>
                    <div class="authors"><b>Clemens Eppner</b> and <a href="https://www.robotics.tu-berlin.de/menue/team/oliver_brock/">Oliver Brock</a></div>
                    <div class="booktitle">Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). Tokyo, Japan. November 2013.</div>
                    <div>[<a href="#" title="Show Abstract" onclick="javascript:togglediv('eppner2013grasping_abstract'); return false;">Abstract</a>] [<a title="Show BibTex" href="#" onclick="javascript:togglediv('eppner2013grasping_bibtex'); return false;">BibTex</a>] [<a href="http://www.robotics.tu-berlin.de/fileadmin/fg170/Publikationen_pdf/eppner_iros13.pdf">PDF</a>] </div>
                    <div class="bibtex" id="eppner2013grasping_bibtex"><br />@inproceedings{eppner2013grasping,<br />&nbsp;&nbsp;title = {Grasping Unknown Objects by Exploiting Shape Adaptability and Environmental Constraints},<br />&nbsp;&nbsp;author = {Eppner, Clemens and Brock, Oliver},<br />&nbsp;&nbsp;booktitle = {Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},<br />&nbsp;&nbsp;year = {2013}<br />}</div>
                    <div class="abstract" id="eppner2013grasping_abstract">In grasping, shape adaptation between hand and object has a major influence on grasp success. In this paper, we present an approach to grasping unknown objects that explicitly considers the effect of shape adaptability to simplify perception. Shape adaptation also occurs between the hand and the environment, for example, when fingers slide across the surface of the table to pick up a small object. Our approach to grasping also considers environmental shape adaptability to select grasps with high probability of success. We validate the proposed shape-adaptability-aware grasping approach in 880 real-world grasping trials with 30 objects. Our experiments show that the explicit consideration of shape adaptability of the hand leads to robust grasping of unknown objects. Simple perception suffices to achieve this robust grasping behavior.</div>
                </div>
            </div>
            
            <div class="publication">
                <div class="col-image"><img src="thumbnails/faber2009humanoid.png" width="100" alt="" /></div>
                <div class="col-rest">
                    <h5>The Humanoid Museum Tour Guide Robotinho</h5>
                    <div class="authors"><a href="https://scholar.google.de/citations?user=2UAQEs8AAAAJ&hl=en">Felix Faber</a>, <a href="https://www.hrl.uni-bonn.de/Members/maren">Maren Bennewitz</a>, <b>Clemens Eppner</b>, <a href="">Attila Gorog</a>, <a href="">Christoph Gonsior</a>, <a href="http://www2.informatik.uni-freiburg.de/~joho/">Dominik Joho</a>, <a href="https://scholar.google.com/citations?user=l_BdzQkAAAAJ&hl=de">Michael Schreiber</a> and <a href="http://ais.uni-bonn.de/behnke/">Sven Behnke</a></div>
                    <div class="booktitle">Proceedings of the 18th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN). Toyama, Japan. September 2009.</div>
                    <div>[<a href="#" title="Show Abstract" onclick="javascript:togglediv('faber2009humanoid_abstract'); return false;">Abstract</a>] [<a title="Show BibTex" href="#" onclick="javascript:togglediv('faber2009humanoid_bibtex'); return false;">BibTex</a>] [<a href="http://www.robotics.tu-berlin.de/fileadmin/fg170/Publikationen_pdf/eppner_oct.2009.pdf">PDF</a>]  [<a href="https://www.youtube.com/watch?v=PF2qZ-O3yAM">Video</a>]</div>
                    <div class="bibtex" id="faber2009humanoid_bibtex"><br />@inproceedings{faber2009humanoid,<br />&nbsp;&nbsp;title = {The Humanoid Museum Tour Guide Robotinho},<br />&nbsp;&nbsp;author = {Faber, Felix and Bennewitz, Maren and Eppner, Clemens and Gorog, Attila and Gonsior, Christoph and Joho, Dominik and Schreiber, Michael and Behnke, Sven},<br />&nbsp;&nbsp;booktitle = {Proceedings of the 18th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)},<br />&nbsp;&nbsp;year = {2009}<br />}</div>
                    <div class="abstract" id="faber2009humanoid_abstract">Wheeled tour guide robots have already been deployed in various museums or fairs worldwide. A key requirement for successful tour guide robots is to interact with people and to entertain them. Most of the previous tour guide robots, however, focused more on the involved navigation task than on natural interaction with humans. Humanoid robots, on the other hand, offer a great potential for investigating intuitive, multimodal interaction between humans and machines. In this paper, we present our mobile full-body humanoid tour guide robot Robotinho. We provide mechanical and electrical details and cover perception, the integration of multiple modalities for interaction, navigation control, and system integration aspects. The multimodal interaction capabilities of Robotinho have been designed and enhanced according to the questionnaires filled out by the people who interacted with the robot at previous public demonstrations. We present experiences we have made during experiments in which untrained users interacted with the robot.</div>
                </div>
            </div>
            
            <div class="publication">
                <div class="col-image"><img src="thumbnails/eppner2009imitation.png" width="100" alt="" /></div>
                <div class="col-rest">
                    <h5>Imitation Learning with Generalized Task Descriptions</h5>
                    <div class="authors"><b>Clemens Eppner</b>, <a href="http://jsturm.de/wp/">Jürgen Sturm</a>, <a href="https://www.hrl.uni-bonn.de/Members/maren">Maren Bennewitz</a>, <a href="https://www.ipb.uni-bonn.de/people/cyrill-stachniss/">Cyrill Stachniss</a> and <a href="http://www2.informatik.uni-freiburg.de/~burgard/">Wolfram Burgard</a></div>
                    <div class="booktitle">Proceedings of the IEEE International Conference on Robotics and Automation (ICRA). Kobe, Japan. May 2009.</div>
                    <div>[<a href="#" title="Show Abstract" onclick="javascript:togglediv('eppner2009imitation_abstract'); return false;">Abstract</a>] [<a title="Show BibTex" href="#" onclick="javascript:togglediv('eppner2009imitation_bibtex'); return false;">BibTex</a>] [<a href="http://www.robotics.tu-berlin.de/fileadmin/fg170/Publikationen_pdf/EppnerMay_2009.pdf">PDF</a>]  [<a href="https://www.youtube.com/playlist?list=PLzPHJWMXrQM4NeNj7KnQFAKrOjAty2LRt">Video</a>]</div>
                    <div class="bibtex" id="eppner2009imitation_bibtex"><br />@inproceedings{eppner2009imitation,<br />&nbsp;&nbsp;title = {Imitation Learning with Generalized Task Descriptions},<br />&nbsp;&nbsp;author = {Eppner, Clemens and Sturm, J{\"u}rgen and Bennewitz, Maren and Stachniss, Cyrill and Burgard, Wolfram},<br />&nbsp;&nbsp;booktitle = {Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)},<br />&nbsp;&nbsp;year = {2009}<br />}</div>
                    <div class="abstract" id="eppner2009imitation_abstract">In this paper, we present an approach that allows a robot to observe, generalize, and reproduce tasks observed from multiple demonstrations. Motion capture data is recorded in which a human instructor manipulates a set of objects. In our approach, we learn relations between body parts of the demonstrator and objects in the scene. These relations result in a generalized task description. The problem of learning and reproducing human actions is formulated using a dynamic Bayesian network (DBN). The posteriors corresponding to the nodes of the DBN are estimated by observing objects in the scene and body parts of the demonstrator. To reproduce a task, we seek for the maximum-likelihood action sequence according to the DBN. We additionally show how further constraints can be incorporated online, for example, to robustly deal with unforeseen obstacles. Experiments carried out with a real 6-DoF robotic manipulator as well as in simulation show that our approach enables a robot to reproduce a task carried out by a human demonstrator. Our approach yields a high degree of generalization illustrated by performing a pick-and-place and a whiteboard cleaning task.</div>
                </div>
            </div>
            

        <h3 class="header">Theses</h3>
        
            <div class="publication">
                <div class="col-image"><img src="thumbnails/eppner2018thesis.png" width="100" alt="" /></div>
                <div class="col-rest">
                    <h5>Robot Grasping by Exploiting Compliance and Environmental Constraints</h5>
                    <div class="authors"><b>Clemens Eppner</b></div>
                    <div class="booktitle">Doctoral Thesis. Technische Universität Berlin. February 2018.</div>
                    <div>[<a href="#" title="Show Abstract" onclick="javascript:togglediv('eppner2018thesis_abstract'); return false;">Abstract</a>] [<a title="Show BibTex" href="#" onclick="javascript:togglediv('eppner2018thesis_bibtex'); return false;">BibTex</a>] [<a href="https://depositonce.tu-berlin.de/bitstream/11303/9884/4/eppner_clemens.pdf">PDF</a>]  [<a href="https://www.youtube.com/playlist?list=PLzPHJWMXrQM7_-AOvx26nX1PDzGLkoYZb">Video</a>]</div>
                    <div class="bibtex" id="eppner2018thesis_bibtex"><br />@phdthesis{eppner2018thesis,<br />&nbsp;&nbsp;title = {Robot Grasping by Exploiting Compliance and Environmental Constraints},<br />&nbsp;&nbsp;author = {Clemens Eppner},<br />&nbsp;&nbsp;booktitle = {Doctoral Thesis},<br />&nbsp;&nbsp;school = {Technische Universität Berlin},<br />&nbsp;&nbsp;year = {2018}<br />}</div>
                    <div class="abstract" id="eppner2018thesis_abstract">Grasping is a crucial skill for any autonomous system that needs to alter the physical world. The complexity of robot grasping stems from the fact that any solution comprises various components: Hand design, control, perception, and planning all affect the success of a grasp. Apart from picking solutions in well-defined industrial scenarios, general grasping in unstructured environment is still an open problem.
In this thesis, we exploit two general properties to devise grasp planning algorithms: the compliance of robot hands and the stiffness of the environment that surrounds an object. We view hand compliance as an enabler for local adaptability in the grasping process that does not require explicit reasoning or planning. As a result, we study compliance-aware algorithms to synthesize grasps. Exploiting hand compliance also simplifies perception, since precise geometric object models are not needed. Complementary to hand compliance is the idea of exploiting the stiffness of the environment. In real-world scenarios, objects never occur in isolation. They are situated in an environmental context: on a table, in a shelf, inside a drawer, etc. Robotic grasp strategies can benefit from contact with the environment by pulling objects to edges, pushing them against surfaces etc. We call this principle the exploitation of environmental constraints. We present grasp planning algorithms which detect and sequence environmental constraint exploitations.
We study the two ideas by focusing on the relationships between the three main constituents of the grasping problem: hand, object, and environment. We show that the interactions between adaptable hands and objects lend themselves to low-dimensional grasp actions. Based on this insight, we devise two grasp planning algorithms which map compliance modes to raw sensor signals using minimal prior knowledge. Next, we focus on the interactions between hand and environment. We show that contacting the environment can improve success in motion and grasping tasks. We conclude our investigations by considering interactions between all three factors: hand, object, and environment. We extend our grasping approach to select the most appropriate environmental constraint exploitation based on the shape of an object. Finally, we consider simple manipulation tasks that require individual finger movements. Although compliant hands pose challenges due to the difficulty in modeling and limitations in sensing, we propose an approach to learn feedback control strategies that solve these tasks. We evaluate all algorithms presented in this thesis in extensive real-world experiments, compare their assumptions and discuss limitations. The investigations and planning algorithms show that exploiting compliance in hands and stiffness in the environment leads to improved grasp performance.</div>
                </div>
            </div>
            
            <div class="publication">
                <div class="col-image"><img src="thumbnails/eppner2008thesis.png" width="100" alt="" /></div>
                <div class="col-rest">
                    <h5>Techniques for the Imitiation of Manipulative Actions by Robots</h5>
                    <div class="authors"><b>Clemens Eppner</b></div>
                    <div class="booktitle">Diploma Thesis. Albert-Ludwigs-Universität Freiburg. November 2008.</div>
                    <div>[<a href="#" title="Show Abstract" onclick="javascript:togglediv('eppner2008thesis_abstract'); return false;">Abstract</a>] [<a title="Show BibTex" href="#" onclick="javascript:togglediv('eppner2008thesis_bibtex'); return false;">BibTex</a>] [<a href="http://www.robotics.tu-berlin.de/fileadmin/fg170/Publikationen_pdf/2008_eppner_thesis.pdf">PDF</a>]  [<a href="https://www.youtube.com/playlist?list=PLzPHJWMXrQM4NeNj7KnQFAKrOjAty2LRt">Video</a>]</div>
                    <div class="bibtex" id="eppner2008thesis_bibtex"><br />@phdthesis{eppner2008thesis,<br />&nbsp;&nbsp;title = {Techniques for the Imitiation of Manipulative Actions by Robots},<br />&nbsp;&nbsp;author = {Clemens Eppner},<br />&nbsp;&nbsp;booktitle = {Diploma Thesis},<br />&nbsp;&nbsp;school = {Albert-Ludwigs-Universität Freiburg},<br />&nbsp;&nbsp;year = {2008}<br />}</div>
                    <div class="abstract" id="eppner2008thesis_abstract">In this thesis, we present an approach that allows a robot to observe, generalize, and reproduce tasks observed from multiple demonstrations. Motion capture data is recorded in which a human instructor manipulates a set of objects. In our approach, we learn relations between body parts of the demonstrator and objects in the scene. These relations result in a generalized task description. The problem of learning and reproducing human actions is formulated using a dynamic Bayesian network (DBN). The posteriors corresponding to the nodes of the DBN are estimated by observing objects in the scene and body parts of the demonstrator. To reproduce a task, we seek for the maximum-likelihood action sequence according to the DBN. We additionally show how further constraints can be incorporated online, for example, to robustly deal with unforeseen obstacles. Experiments carried out with a real 6-DoF robotic manipulator as well as in simulation show that our approach enables a robot to reproduce a task carried out by a human demonstrator. Our approach yields a high degree of generalization illustrated by performing a pick-and-place, a pouring, and a whiteboard cleaning task.</div>
                </div>
            </div>
            
            <div class="publication">
                <div class="col-image"><img src="thumbnails/eppner2007thesis.png" width="100" alt="" /></div>
                <div class="col-rest">
                    <h5>Simulation eines Prallluftschiffs</h5>
                    <div class="authors"><b>Clemens Eppner</b></div>
                    <div class="booktitle">Studienarbeit. Albert-Ludwigs-Universität Freiburg. October 2007.</div>
                    <div>[<a href="#" title="Show Abstract" onclick="javascript:togglediv('eppner2007thesis_abstract'); return false;">Abstract</a>] [<a title="Show BibTex" href="#" onclick="javascript:togglediv('eppner2007thesis_bibtex'); return false;">BibTex</a>] [<a href="papers/eppner2007thesis.pdf">PDF</a>] </div>
                    <div class="bibtex" id="eppner2007thesis_bibtex"><br />@phdthesis{eppner2007thesis,<br />&nbsp;&nbsp;title = {Simulation eines Prallluftschiffs},<br />&nbsp;&nbsp;author = {Clemens Eppner},<br />&nbsp;&nbsp;booktitle = {Studienarbeit},<br />&nbsp;&nbsp;school = {Albert-Ludwigs-Universität Freiburg},<br />&nbsp;&nbsp;year = {2007}<br />}</div>
                    <div class="abstract" id="eppner2007thesis_abstract"></div>
                </div>
            </div>
            

        <h3 class="header">Patents</h3>
        
            <div class="publication">
                <div class="col-image"><img src="thumbnails/mousavian2021grasp.png" width="100" alt="" /></div>
                <div class="col-rest">
                    <h5>Grasp determination for an object in clutter</h5>
                    <div class="authors"><a href="https://cs.gmu.edu/~amousavi/">Arsalan Mousavian</a>, <b>Clemens Eppner</b> and <a href="https://homes.cs.washington.edu/~fox/">Dieter Fox</a></div>
                    <div class="booktitle">US20210138655A1. United States. May 2021.</div>
                </div>
            </div>
            
            <div class="publication">
                <div class="col-image"><img src="thumbnails/mousavian2020grasp.png" width="100" alt="" /></div>
                <div class="col-rest">
                    <h5>Grasp generation using a variational autoencoder</h5>
                    <div class="authors"><a href="https://cs.gmu.edu/~amousavi/">Arsalan Mousavian</a>, <b>Clemens Eppner</b> and <a href="https://homes.cs.washington.edu/~fox/">Dieter Fox</a></div>
                    <div class="booktitle">US20200361083A1. United States. November 2020.</div>
                </div>
            </div>
            
    </section>

    <section class="container linklist">
        <a name="links"></a>
        <h3 class="header">Links</h3>
        These resources might be helpful to navigate the exciting and muddy waters of academia, with a bias towards the
        field of computer science and robotics.
        <h5>
    <a href="#academic-writing-links" target="_self" aria-hidden="true" class="aal_anchor"
        id="academic-writing-links"><svg aria-hidden="true" class="aal_svg" height="16" version="1.1"
            viewBox="0 0 16 16" width="16">
            <path fill-rule="evenodd"
                d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z">
            </path>
        </svg></a>
    Academic Writing
</h5>
<ul>
    <li><strong>Papers</strong>
        <ul>
            <li>Alberto Rodriguez: <a href="https://youtu.be/pq4qmFYcn4E">Good Practices for Good Writing</a> (from the
                R:SS 2020 workshop <a href="https://sites.google.com/view/rss20-gcr">Citizens of Robotics Research</a>)
            </li>
            <li>Amy Tabb: <a href="https://amytabb.com/ts/2019_05_09">Basic Submission Checklist</a></li>
            <li>Bill Freeman: <a href="https://youtu.be/MKUCz_3Ee0A?t=248">How to Write a Good Research Paper</a> [<a
                    href="https://www.cc.gatech.edu/~parikh/citizenofcvpr/static/slides/freeman_how_to_write_papers.pdf">Slides</a>]
                (from the CVPR 2018 workshop <a href="https://www.cc.gatech.edu/~parikh/citizenofcvpr/">Good Citizen of
                    CVPR</a>)</li>
            <li>Brett Mensh and Konrad Kording: <a
                    href="https://journals.plos.org/ploscompbiol/article/file?type=printable&id=10.1371/journal.pcbi.1005619">Ten
                    Simple Rules for Structuring Papers</a></li>
            <li>Cormac McCarthy: <a href="https://www.nature.com/articles/d41586-019-02918-5">Novelist Cormac McCarthy’s
                    Tips on How to Write a Great Science Paper</a></li>
            <li>Devi Parikh: <a href="https://medium.com/@deviparikh/planning-paper-writing-553f497e8839">Planning Paper
                    Writing</a></li>
            <li>George Whitesides: <a href="https://youtu.be/q3mrRH2aS98">How to Write a Paper to Communicate Your
                    Research</a></li>
            <li>Jitendra Malik: <a href="https://www.youtube.com/watch?v=imEtTnQKt4M">How to Write a Good Paper</a> [<a
                    href="https://www.cc.gatech.edu/~parikh/citizenofcvpr/static/slides/malik_write_good_paper.pdf">Slides</a>]
                (from the CVPR 2018 workshop <a href="https://www.cc.gatech.edu/~parikh/citizenofcvpr/">Good Citizen of
                    CVPR</a>)</li>
            <li>Kate Saenko: <a
                    href="https://docs.google.com/presentation/d/1PZj0Sev2yjDu9NNr96S_wwjKCgIDhGmLjW1vtQpDhlk/edit#slide=id.p">How
                    to Write the Introduction for a Research Paper</a></li>
            <li>Kevin P. Lee: <a href="https://web.cs.ucdavis.edu/~amenta/w10/writingman.pdf">A Guide to Writing
                    Mathematics</a></li>
            <li>Larry McEnerney: <a href="https://youtu.be/vtIzMaLkCaM">Leadership Lab: The Craft of Writing
                    Effectively</a></li>
            <li>Marcel Hofeditz: <a
                    href="https://medium.com/@MarcelHofeditz/5-academic-writing-tips-for-non-native-speaker-57168bea3db2">5
                    Academic Writing Tips for Non-Native Speakers</a></li>
            <li>Pratap Tokekar: <a href="http://tokekar.github.io/docs/Tokekar-WritingPapers-Talk.pdf">Thoughts on
                    Writing a Good (Robotics) Paper</a></li>
            <li>Simon Peyton Jones: <a
                    href="https://www.microsoft.com/en-us/research/academic-program/write-great-research-paper/">How to
                    Write a Great Research Paper</a> (<a href="https://youtu.be/VK51E3gHENc">Talk</a>)</li>
            <li>Stefanie Tellex: <a href="https://h2r.cs.brown.edu/writing-a-technical-paper/">Writing a Technical
                    Paper</a></li>
            <li>Virginia Gewin: <a href="https://www.nature.com/articles/d41586-018-02404-4">How to Write a First-Class
                    Paper</a></li>
            <li>Zachary Lipton: <a
                    href="http://approximatelycorrect.com/2018/01/29/heuristics-technical-scientific-writing-machine-learning-perspective/">Heuristics
                    for Scientific Writing (a Machine Learning Perspective)</a></li>
        </ul>
    </li>
    <li><strong>Abstracts</strong>
        <ul>
            <li>Chittaranjan Andrade: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3136027/">How to Write a
                    Good Abstract for a Scientific Paper or Conference Presentation</a></li>
            <li>Juliana Carvalho Ferreira and Cecilia Maria Patino: <a
                    href="http://www.scielo.br/scielo.php?script=sci_arttext&pid=S1806-37132018000400260">Twelve Tips to
                    Write an Abstract for a Conference: Advice for Young and Experienced Investigators</a></li>
            <li>Nadine J. Kabengi: <a
                    href="https://www.acsmeetings.org/files/meetings/tips-for-writing-abstracts-annual-mtgs.pdf">Tips
                    for Writing an Abstract</a></li>
            <li>Nature 435, 114-118 (2005): <a
                    href="http://www.markowetzlab.org/skills/How-to-write-a-Nature-abstract.pdf">How to Construct a
                    Nature Summary Paragraph</a></li>
            <li>Philip Koopman: <a href="https://users.ece.cmu.edu/~koopman/essays/abstract.html">How to Write an
                    Abstract</a></li>
            <!--    <li>Adriano Aguzzi: <a href="https://twitter.com/AguzziTemp/status/1153307597921017857/photo/1">AA's Guide on How to Write an Effective Abstract</a></li> -->
        </ul>
    </li>
    <li><strong>Reviews</strong>
        <ul>
            <li>Amy Tabb: <a href="https://amytabb.com/ts/2019_06_09">My Reviewing Style, or How to Review Technical
                    Papers When You've Not Been Taught How</a></li>
            <li>Devi Parikh, Dhruv Battra, and Stefan Lee: <a
                    href='https://medium.com/@deviparikh/how-we-write-rebuttals-dc84742fece1'>How We Write Rebuttals</a>
            </li>
            <li>EMNLP Committee Blog: <a href='https://2020.emnlp.org/blog/2020-05-17-write-good-reviews'>Advice on
                    Reviewing for EMNLP</a></li>
            <li>Seth Hutchinson: <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5663683">Surviving the
                    Review Process</a></li>
        </ul>
    </li>
    <li><strong>Research Statements</strong>
        <ul>
            <li>Carnegie Mellon University, Global Communication Center: <a
                    href="https://www.cmu.edu/gcc/handouts-and-resources/handouts/research-statement.pdf">Writing A
                    Research Statement</a></li>
        </ul>
    </li>
    <li><strong>Research Proposals</strong>
        <ul>
            <li>George A. Hazelrigg: <a href="https://www.cs.rpi.edu/~trink/HazelriggWinningResearchProposal.pdf">Twelve
                    Steps to a Winning Research Proposal</a></li>
            <li>George A. Hazelrigg: <a
                    href="http://poole.ncsu.edu/i/com/weblogs/research-development/Honing-Proposal-Skillls-1.pdf">Honing
                    Your Proposal Writing Skills</a></li>
            <li>Jeff Trinkle: <a href="https://www.cs.rpi.edu/~trink/proposal-preparation.html">Some Keys to Preparing a
                    Competitive Proposal From My Observations</a></li>
        </ul>
    </li>
    <li><strong>Posters</strong>
        <ul>
            <li>Amy Tabb: <a href="https://amytabb.com/ts/2019_11_23/">Poster Session Strategies</a></li>
        </ul>
    </li>
    <li><strong>LaTeX</strong>
        <ul>
            <li>Devi Parikh: <a
                    href="https://medium.com/@deviparikh/shortening-papers-to-fit-page-limits-97601318681d">Shortening
                    Papers to Fit Page Limits</a></li>
	    <li>Dustin Tran: <a
		    href="https://twitter.com/dustinvtran/status/1398129705660805121">Better LaTeX Defaults</a></li>
            <li>Jia-Bin Huang: <a
                    href="https://filebox.ece.vt.edu/~jbhuang/slides/Research%20101%20-%20Paper%20Writing%20with%20LaTeX.pdf">Paper
                    Writing with LaTeX</a></li>
            <li>Markus Püschel: <a href="https://inf.ethz.ch/personal/markusp/teaching/guides/guide-tables.pdf">Small
                    Guide to Making Nice Tables</a></li>
            <li>Noah Snavely: <a href="http://bit.ly/latex-style">A Short LaTeX Style Guide</a></li>
        </ul>
    </li>
</ul>
<h5>
    <a href="#research-links" target="_self" aria-hidden="true" class="aal_anchor" id="research-links"><svg
            aria-hidden="true" class="aal_svg" height="16" version="1.1" viewBox="0 0 16 16" width="16">
            <path fill-rule="evenodd"
                d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z">
            </path>
        </svg></a>
    Research
</h5>
<ul>
    <li><strong>General Advice</strong>
        <ul>
            <li>Academia Stack Exchange: <a
                    href="https://academia.stackexchange.com/questions/tagged/research-process?tab=Votes">Posts Tagged
                    With [research-process]</a></li>
            <li>Chris Olah: <a href="https://colah.github.io/notes/taste/">Research Taste Exercises</a></li>
            <li>Colin Chandler: <a
                    href="https://pdfs.semanticscholar.org/2d64/e940fad5645c1479f249e9c368d4976e0f01.pdf">What Is the
                    Meaning of Impact in Relation to Research and Why Does It Matter? A View From Inside Academia</a>
            </li>
            <li>Cordelia Schmid: <a href="https://youtu.be/imEtTnQKt4M?t=1305">How to Do Good Research & Evaluation</a>
                [<a
                    href="https://www.cc.gatech.edu/~parikh/citizenofcvpr/static/slides/schmid_good_research_and_evaluation.pdf">Slides</a>]
                (from the CVPR 2018 workshop <a href="https://www.cc.gatech.edu/~parikh/citizenofcvpr/">Good Citizen of
                    CVPR</a>)</li>
            <li>David Epstein: <a href="https://youtu.be/B6lBtiQZSho">Why Specializing Early Doesn't Always Mean Career
                    Success</a></li>
            <li>Danica Kragic: <a href="https://youtu.be/sSCK3oYGOo4">Building an Academic Career</a> [<a
                    href="https://drive.google.com/file/d/1WbhSoua3aFPW920DMG3Sm3OEzK-TeOIx/view?usp=sharing">Slides</a>]
                (from the R:SS 2020 workshop <a href="https://sites.google.com/view/rss20-gcr">Citizens of Robotics
                    Research</a>)</li>
            <li>David Patterson: <a href="https://people.eecs.berkeley.edu/~pattrsn/talks/BadCareer.pdf">How to Have a
                    Bad Career in Research/Academia</a></li>
            <li>Donald Knuth: <a href="https://youtu.be/75Ju0eM5T2c">My Advice to Young People</a> and <a
                    href="https://shuvomoy.github.io/blogs/posts/Knuth-on-work-habits-and-problem-solving-and-happiness/">
                    On work habits, problem solving, and happiness</a> (by Shuvomoy Das Gupta)</li>
            <li>Eugene Vinitsky: <a href="https://rlblogging.notion.site/Personal-Rules-of-Productive-Research-44a456bacf7c4805a9ea417b9d3ab1b3">Personal
                    Rules of Productive Research</a></li>
            <li>H.T. Kung: <a href="https://www.eecs.harvard.edu/htk/phdadvice/">Useful Thoughts about Research</a></li>
            <li>Jeff Trinkle: <a href="https://www.cs.rpi.edu/~trink/research-directions.html">Identifying a Fundable
                    Research Project</a></li>
            <li>Jia-Bin Huang: <a href="https://twitter.com/jbhuang0604/status/1423499757591400448">How to Come Up With 
                    Research Ideas?</a></li>
            <li>Jim Kurose: <a href="http://www-net.cs.umass.edu/kurose/talks/student_keynote_final.pdf">15 Pieces of
                    Advice I Wish My PhD Advisor Had Given Me</a></li>
            <li>John Schulman: <a href="http://joschu.net/blog/opinionated-guide-ml-research.html">An Opinionated Guide
                    to ML Research</a></li>
            <li>Mark Dredze and Hanna M. Wallach: <a
                    href="https://people.cs.umass.edu/~wallach/how_to_be_a_successful_phd_student.pdf">How to Be a
                    Successful PhD Student</a></li>
            <li>Michael Nielsen: <a href="https://michaelnielsen.org/blog/principles-of-effective-research/">Principles
                    of Effective Research</a></li>
            <li>Mor Naaman: <a href="https://stechlab.github.io/phd-syllabus/">A Syllabus for PhD Students Working in My
                    Group</a></li>
            <li>Nick Feamster and Alex Gray: <a href="https://greatresearch.org/">How to Do Great Research</a></li>
            <li>Remzi Arpaci-Dusseau: <a href="https://youtu.be/fqPSnjewkuA">Graduate School: Keys To Success</a></li>
            <li>Richard Hamming: <a href="https://youtu.be/a1zDuOPkMSw">You and Your Research</a> [<a
                    href="https://www.cs.virginia.edu/~robins/YouAndYourResearch.html">Transcript</a>]</li>
            <li>Rosanne Liu: <a href="https://youtu.be/0blQp0_9NwY">AI Research: The Unreasonably Narrow Path and How not 
                    to be Miserable</a></li>
            <li>Rowan McAllister: <a href="https://rowanmcallister.github.io/post/workshops/">Workshop Organization Guide
                    </a></li>
            <li>Siddhartha Srinivasa: <a href="https://youtu.be/Zz3bb3OWl68">What Should I Work On?</a> (from the R:SS
                2020 workshop <a href="https://sites.google.com/view/rss20-gcr">Citizens of Robotics Research</a>)</li>
            <li>Vincent Vanhoucke: <a
                    href="https://medium.com/s/story/so-you-want-to-be-a-research-scientist-363c075d3d4c">So You Want to
                    Be a Research Scientist</a></li>
            <li>Vladlen Koltun: <a href="https://youtu.be/4LEZED1YXm0?t=1420">Doing (Good) Research</a> [<a
                    href="https://www.cc.gatech.edu/~parikh/citizenofcvpr/static/slides/koltun_doing_(good)_research.pdf">Slides</a>]
                (from the CVPR 2018 workshop <a href="https://www.cc.gatech.edu/~parikh/citizenofcvpr/">Good Citizen of
                    CVPR</a>)</li>
        </ul>
    </li>
    <li><strong>Designing Experiments</strong>
        <ul>
            <li>Anca Dragan: <a href="https://youtu.be/S0SbrUKcedQ">Intro to Experiment Design</a> [<a
                    href="https://drive.google.com/file/d/1wfwxFj2BTuiNmKBhrMEIQoAs7BxNHUkK/view?usp=sharing">Slides</a>]
                (from the R:SS 2020 workshop <a href="https://sites.google.com/view/rss20-gcr">Citizens of Robotics
                    Research</a>)</li>
            <li>Andy Cockburn, Pierre Dragicevic, Lonni Besançon, and Carl Gutwin: <a
                    href="https://cacm.acm.org/magazines/2020/8/246369-threats-of-a-replication-crisis-in-empirical-computer-science/fulltext">Threats
                    of a Replication Crisis in Empirical Computer Science</a> (<a
                    href="https://youtu.be/wp9B2P4jC-I">Talk</a>)</li>
            <li>Peter Norvig: <a href="https://norvig.com/experiment-design.html">Warning Signs in Experimental Design and
                    Interpretation</a></li>
        </ul>
    </li>
    <li><strong>Making Videos</strong>
        <ul>
            <li>Stefanie Tellex, Eric Rosen, and George Konidaris: <a href="https://youtu.be/fRxgqS8tJhQ">Best Practices
                    for Robot Videos</a></li>
            <li>Travis Deyle: <a
                    href="http://www.hizook.com/blog/2012/07/02/being-honest-robot-videos-motion-capture-speedup-rates-and-teleoperation">Being
                    Honest in Robot Videos: Motion Capture, Speedup Rates, and Teleoperation</a></li>
        </ul>
    </li>
    <li><strong>Reading Papers</strong>
        <ul>
            <li>Alec Jacobson and Colin Raffel: <a href="https://colinraffel.com/blog/role-playing-seminar.html">Role-Playing Paper-Reading
                    Seminars</a></li>
            <li>Andrew Ng: <a href="https://youtu.be/733m6qBH-jI">Career Advice / Reading Research Papers</a></li>
            <li>David Ha: <a href="https://twitter.com/hardmaru/status/1305758751798910976">How Do You Skim a Research
                    Paper?</a></li>
            <li>Eric Jang: <a href="https://blog.evjang.com/2021/01/understanding-ml.html">How to Understand ML Papers Quickly</a></li>
            <li>Jason Eisner: <a href="https://www.cs.jhu.edu/~jason/advice/how-to-read-a-paper.html">How to Read a
                    Technical Paper</a></li>
            <li>Srinivasan Keshav: <a href="http://ccr.sigcomm.org/online/files/p83-keshavA.pdf">How to Read a Paper</a>
            </li>
            <li>Yannic Kilcher: <a href='https://youtu.be/Uumd2zOOz60'>How I Read a Paper: Facebook's DETR</a></li>
        </ul>
    </li>
    <li><strong>Giving Talks</strong>
        <ul>
            <li>Kayvon Fatahalian: <a href="https://graphics.stanford.edu/~kayvonf/misc/cleartalktips.pdf">Tips for
                    Giving Clear Talks</a>
            </li>
            <li>Kristen Grauman: <a href="https://www.youtube.com/watch?v=4LEZED1YXm0">Tips for Preparing a Clear
                    Talk</a> [<a
                    href="https://www.cc.gatech.edu/~parikh/citizenofcvpr/static/slides/grauman_preparing_clear_talks.pdf">Slides</a>]
                (from the CVPR 2018 workshop <a href="https://www.cc.gatech.edu/~parikh/citizenofcvpr/">Good Citizen of
                    CVPR</a>)</li>
            <li>Patrick Winston: <a href="https://youtu.be/Unzc731iCUY">How To Speak</a> (from his MIT course: <a
                    href="https://ocw.mit.edu/how_to_speak">https://ocw.mit.edu/how_to_speak</a>)</li>
            <li>Wolfram Burgard: <a href="https://youtu.be/JQaNkmWzArU">How to Give a Presentation</a> (from the R:SS
                2020 workshop <a href="https://sites.google.com/view/rss20-gcr">Citizens of Robotics Research</a>)</li>
        </ul>
    </li>
    <li><strong>Thinking Patterns and Unsolicited Advice</strong>
        <ul>
            <li>Edward B. Burger and Michael Starbird: <a
                    href="https://www.goodreads.com/book/show/14891980-the-5-elements-of-effective-thinking">The 5
                    Elements of Effective Thinking</a></li>
            <li>Gabriel Weinberg: <a
                    href="https://medium.com/@yegg/mental-models-i-find-repeatedly-useful-936f1cc405d">Mental Models I
                    Find Repeatedly Useful</a></li>
            <li>George Mack: <a href="https://twitter.com/george__mack/status/1350513143387189248">15 of the most useful
                    razors and
                    rules I've found</a></li>
            <li>Jesse Richardson, Andy Smith, Som Meaden, and Flip Creative: <a
                    href="https://yourlogicalfallacyis.com">Thou Shalt not Commit Logical Fallacies</a></li>
            <li>Kevin Kelly: <a href='https://kk.org/thetechnium/68-bits-of-unsolicited-advice/'>68 Bits of Unsolicited
                Advice</a>, <a href='https://kk.org/thetechnium/99-additional-bits-of-unsolicited-advice/'>99 Additional
                Bits of Unsolicited Advice</a></li>
            <li>Nabeel Qureshi: <a href='https://nabeelqu.co/understanding'>How to Understand Things</a></li>
            <li>Rhiannon Beaubien and Shane Parrish: <a href='https://fs.blog/mental-models/'>Mental Models (~100 Models Explained)</a>
            </li>
            <li>Wikipedia: <a href="https://en.wikipedia.org/wiki/List_of_cognitive_biases">List of Cognitive Biases</a>
            </li>
            <li>Wikipedia: <a href="https://en.wikipedia.org/wiki/List_of_fallacies">List of Fallacies</a></li>
            <li>Wikipedia: <a href="https://en.wikipedia.org/wiki/Five_whys">Five whys</a></li>
        </ul>
    </li>
    <li><strong>Communication</strong>
        <ul>
            <li>Basecamp: <a href="https://basecamp.com/guides/how-we-communicate">How We Communicate</a></li>
            <li>Basecamp: <a href="https://basecamp.com/guides/group-chat-problems">Group Chat Problems</a></li>
            <li>Paul Graham: <a href="http://www.paulgraham.com/makersschedule.html">Maker's Schedule, Manager's
                    schedule</a></li>
        </ul>
    </li>
    <li><strong>Leaving Academia</strong>
        <ul>
            <li>Marcel Haas: <a href='http://www.marcelhaas.com/index.php/2018/03/30/leaving-the-field-becoming-an-extronomer/'>Leaving the field:
                    becoming an extronomer</a> and <a href='http://www.marcelhaas.com/index.php/2020/12/16/i-regret-quitting-astrophysics/'>I regret
                    quitting astrophysics</a></li>
            <li>Rodney Brooks: <a href='https://youtu.be/FGbYdsaX5pw'>Robotics Research Methodology Aimed at Start Ups rather than Academia</a></li>
            <li>Vincent Vanhoucke: <a
                    href='https://bcb9f395-a-8ac90a7d-s-sites.googlegroups.com/a/vanhoucke.com/vincent/publications/vanhoucke-phdforum20.pdf'>Research
                    and Engineering Careers for PhDs in Industry</a></li>
            <li>Zachary Ernst: <a
                    href='https://medium.com/@zacernst/leaving-academia-for-the-private-sector-seven-years-later-fbb7849182f6'>Leaving
                    Academia for the Private Sector: Seven Years Later</a></li>
        </ul>
    </li>
</ul>
<!-- https://phdadvice.carrd.co/ -->
    </section>


    <footer>&copy; 2019-2022 Clemens Eppner. <a
            href="http://creativecommons.org/licenses/by-nc-sa/4.0/">CC-NC-SA-4.0</a>. Built with <a
            href="https://github.com/sciunto-org/python-bibtexparser">python-bibtexparser</a>, <a
            href="https://github.com/pallets/jinja">Jinja</a>, and <a href="https://kbrsh.github.io/wing">Wing</a>.
    </footer>
</body>

</html>